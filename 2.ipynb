{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Using \"43_sent_data_sept_22\" csv\n",
        "- Tokenize the sentence\n",
        "- Remove Stop Words\n",
        "- Perform Lemmatization\n",
        "- Extract Entities and Relations\n"
      ],
      "metadata": {
        "id": "tfQpOd-rEHRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Uploading the csv data-file containing the refined Sentences related to PM-JAY as Pandas Dataframe from the Google Drive\n"
      ],
      "metadata": {
        "id": "oxoUMC5rLosk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#data= pd.read_csv(\"/content/drive/MyDrive/Project/43_sent_data_sept_22.csv\")\n",
        "data= pd.read_csv(\"/content/drive/MyDrive/Project/refined_sentences.csv\")"
      ],
      "metadata": {
        "id": "sr4d9SR-vMvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing the spacy library\n",
        "The **displacy** helps to render the dependencies of each sentences\\\n",
        "loading \"**en_core_web_sm**\" will give us the NLP model to carry-on NLP on sentences."
      ],
      "metadata": {
        "id": "BPBszZBsMHZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "ATYwBvk_uiJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "nlp= spacy.load(\"en_core_web_sm\")\n",
        "#nlp= spacy.load(\"en_core_web_lg\")\n",
        "#for sent in data['sentences']:\n",
        "#    print(sent)"
      ],
      "metadata": {
        "id": "TAjxlUO4uqca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Printing the Text, POS, Tag, Depencency and Head for each token (word) in ever sentence. Can be used to find the logic for extraction of entities and relations."
      ],
      "metadata": {
        "id": "KsTwqW2rMyPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "i=1\n",
        "for sent in data['sentences']:\n",
        "    table_data=[]\n",
        "    d= nlp(sent)\n",
        "    for tok in d:\n",
        "        table_data.append([tok.text, tok.pos_,tok.tag_,tok.dep_,tok.head])\n",
        "    print('\\033[1m',i,')'+sent+'\\033[0m')        \n",
        "    print(tabulate(table_data, headers=['Token_text','POS','Tag','Dependency','Head']))\n",
        "    print('\\n')\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "BXgoSihy-UWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t1=\"The EHCP provides the relevant medication and diagnostics as per the package upto 15 days to the PM-JAY Beneficiary\"\n",
        "t2=\"The EHCP provide the relevant medication and diagnostics as per the package upto 15 days to the PM-JAY Beneficiary\"\n",
        "f= nlp(t1)\n",
        "g= nlp(t2)\n",
        "table_data2=[]\n",
        "for tok in f:\n",
        "    table_data2.append([tok.text, tok.pos_,tok.tag_,tok.dep_,tok.head])\n",
        "print('\\033[1m'+sent+'\\033[0m')        \n",
        "print(tabulate(table_data2, headers=['Token_text','POS','Tag','Dependency','Head']))\n",
        "print('\\n')\n",
        "table_data2=[]\n",
        "for tok in g:\n",
        "    table_data2.append([tok.text, tok.pos_,tok.tag_,tok.dep_,tok.head])\n",
        "print('\\033[1m'+sent+'\\033[0m')        \n",
        "print(tabulate(table_data2, headers=['Token_text','POS','Tag','Dependency','Head']))\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "YB1_61ftwaYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now since we have the dependencies of each and every word, we need to extract entities and relations based on these depedencies. \n",
        "## What are to be extracted are as follows :-\n",
        "**1.   Source Entity:-** There can be one or more source entity/ies depending upon the sentence. Usually the source entity will be the found as the first nsubj dependencies. If there is a complex sentence, the next source source entity usually is the first Target Entity and so on.\\\n",
        "**2.   Target Entity:-** There can be one or more Target Entity/ies depending upon the complexity of the sentence. The Target Entity is usually determined by dobj or pobj.\\\n",
        "**3.   Relation:-** There can be only one relation between a Source and a Target Entity, but in a sentence there can be one or more relations depending upon the number of source and target pairs. The relations are usually the ROOT and prep.\\\n",
        "\\\n",
        "**We do need to concatinate the dets, modifiers etc with their respective nsubj, dobj or pobj**\n",
        "\\\n",
        "##Firstly, find the logic to Extract the Source Entity\n",
        "##Secondly, find the logic to extract Relation\n",
        "##Thirdly, find the logic to extract Target Entity \n",
        "**Do keep in mind that the entities and relation must be extracted as a triple for each sentence !!!**\n"
      ],
      "metadata": {
        "id": "omesna2jBde9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to extract Triple (Source, Relation and Target) from each sentence\n",
        "#firstly find the ROOT along with its prefixes and suffixes\n",
        "#secondly find the Source Entity and the Target Phrase which may contain in itself one or more set of SE,R,TE\n",
        "def getTriple(sent):\n",
        "    se=\"\" #Source Entity\n",
        "    r=\"\" #Relation\n",
        "    te=\"\" #Target Entity\n",
        "########################################################\n",
        "    root_prefix=\"\"\n",
        "    root_suffix=\"\"\n",
        "    root_word=\"\"\n",
        "    combined_word=\"\"\n",
        "    prev_word=[]\n",
        "    for tok in nlp(sent):\n",
        "        if tok.dep_.find('nsubj') != -1: #Found the subject of the sentence\n",
        "            if len(combined_word)==0:\n",
        "                se= tok.text\n",
        "            else:\n",
        "                se= combined_word+\" \"+tok.text\n",
        "            combined_word= \"\"\n",
        "        else:\n",
        "            combined_word= combined_word+\" \"+tok.text\n",
        "        \n",
        "    se=se.strip()\n",
        "    r=r.strip()\n",
        "    te=te.strip()\n",
        "    return([se,r,te])\n"
      ],
      "metadata": {
        "id": "42qOIOqPUf5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting set of triples (soruce, relation and target)\n",
        "triple=[]\n",
        "i=1\n",
        "for sent in data['sentences']:\n",
        "    triple.append(getTriple(sent))\n",
        "for t in triple:\n",
        "    print(i,\")\",t)\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "wI492BTNTw9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for each sentence get token, pos, tag, dependency and head\n",
        "from pathlib import Path\n",
        "from tabulate import tabulate\n",
        "import pandas as pd\n",
        "file_name='Sentence_Details.txt'\n",
        "output_path = Path(\"/content/drive/MyDrive/Project/\" + file_name)\n",
        "with open(output_path,\"w\") as external_file:\n",
        "    i=1\n",
        "    for sent in data['sentences']:\n",
        "        table_data=[]\n",
        "        d= nlp(sent)\n",
        "        for tok in d:\n",
        "            table_data.append([tok.text,tok.pos_,tok.tag_,tok.dep_,tok.head])\n",
        "        #print('\\033[1m'+sent+'\\033[0m', file=external_file)\n",
        "        print(i,\")\\t\",sent, file=external_file)\n",
        "        print(tabulate(table_data,headers=['Token','POS','Tag','Dependency','Head']),file=external_file)\n",
        "        print('\\n',file=external_file)\n",
        "        i+=1\n",
        "    external_file.close()\n",
        "    #df= pd.DataFrame(table_data, columns=['Token','POS','Tag','Dependency','Head'])\n",
        "    #print(df)"
      ],
      "metadata": {
        "id": "TVO-U4k7-6GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t= \"The PMAM informs about the entitlements from the EHCP to the PM-JAY Beneficiary\"\n",
        "t2= \"The SHA/NHA studies the feedback from the PMAM\"\n",
        "d= nlp(t)\n",
        "e=nlp(t2)\n",
        "displacy.render(d, style='dep', jupyter= True)\n",
        "for tok in d:\n",
        "    #print(tok.text)\n",
        "    print('{0}\\t\\t\\t|{1}|{2}|{3}|{4}'.format(tok.text,tok.tag_,tok.dep_,tok.head,spacy.explain(tok.tag_)))\n",
        "    #print(tok.text,\":(tag)\",tok.tag_,\"->\",spacy.explain(tok.dep_),\":(dep)\",tok.dep_,\":(head)\",tok.head)\n",
        "#displacy.render(e, style='dep', jupyter= True)\n",
        "#for tok in e:\n",
        "#    print(tok.text,\":(tag) \",tok.tag_,\":(dep) \",tok.dep_,\":(head) \",tok.head)"
      ],
      "metadata": {
        "id": "fVLBeZ1-sPrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pick up from here\n",
        "#Make Entity and Relation Extractor from the dependencies extracted from the text\n",
        "#These dependencies are saved as images in GDrive Have a look at them\n",
        "\n",
        "#if the dependency is nsubj then extract it along with its components as Source entity\n",
        "#if the dependency is ROOT the extract it along with its components as Relation\n",
        "#if the dependency is dobj then extract it along with its components as First Target Entity\n",
        "#if the dependency is pobj then extract it along with its components as Second Target Entity"
      ],
      "metadata": {
        "id": "TPNNCFU8WRNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving the dependency in each sentence as a image in the drive\n",
        "#from pathlib import Path\n",
        "#i=1\n",
        "for sent in data['sentences']:\n",
        "    d= nlp(sent)\n",
        "    svg= displacy.render(d,style='dep',jupyter=True)\n",
        "    #file_name = str(i) + \".svg\"\n",
        "    #output_path = Path(\"/content/drive/MyDrive/Project/images/\" + file_name)\n",
        "    #output_path.open(\"w\", encoding=\"utf-8\").write(svg)\n",
        "    #i= i+1"
      ],
      "metadata": {
        "id": "KVGvY5EIS5Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''#Tokenize each sentence, remove punctuations, lemmatize and lower case\n",
        "#Get Refined Sentences\n",
        "refined_sent_list=[]\n",
        "punctuations= {\".\",\",\",\"\\n\",\" \"}\n",
        "for each_sent in data['sentences']:\n",
        "    d= nlp(each_sent)\n",
        "    refined_sent=\"\"\n",
        "    for token in d:\n",
        "        if (token.text not in punctuations):\n",
        "            #refined_sent= refined_sent+((token.lemma_).lower())+\" \" #lemmatize and lower case\n",
        "            #refined_sent= refined_sent+((token.text).lower())+\" \" #lowercase\n",
        "            refined_sent= refined_sent+(token.text)+\" \" #no changes\n",
        "    refined_sent= refined_sent.strip()\n",
        "    refined_sent_list.append(refined_sent)\n",
        "for i in refined_sent_list:\n",
        "    print(i)'''"
      ],
      "metadata": {
        "id": "BYwiS-EAy-cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yhWoK-W7SkPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''from spacy import displacy\n",
        "#Check for each sentence the POS tagging and the dependencies\n",
        "i= refined_sent_list[0]\n",
        "print(i)\n",
        "d= nlp(i)\n",
        "for tok in d:\n",
        "    print(tok.text,\"->\",tok.pos_,\"->\",tok.tag_,\":\",spacy.explain(tok.tag_))'''"
      ],
      "metadata": {
        "id": "t3R3hXWIBKpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''displacy.render(d, style='dep', jupyter=True)\n",
        "for tok in d:\n",
        "    print(tok.text,\"--->\",tok.dep_)'''"
      ],
      "metadata": {
        "id": "sRCkCHL5DX_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''t=\"The Patient approaches the EHCP Registration Desk\"\n",
        "d2= nlp(t)\n",
        "displacy.render(d2, style='dep', jupyter=True)\n",
        "for tok in d2:\n",
        "    print(tok.text,\"--->\",tok.dep_,\":\",spacy.explain(tok.dep_))\n",
        "#print(\"Entities:\")\n",
        "#for ent in d2.ents:\n",
        "#    print(ent.text,\": \",ent.label_,\": \",spacy.explain(ent.label_))'''"
      ],
      "metadata": {
        "id": "yT5w-v3WD2FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''for sent in refined_sent_list:\n",
        "    d= nlp(sent)\n",
        "    for tok in d:\n",
        "        print(tok.text,\"->\",tok.pos_,\"->\",tok.tag_,\":\",spacy.explain(tok.tag_))\n",
        "    for ent in d.ents:\n",
        "        print(ent,\": \",ent.label_,\": \",spacy.explain(ent.label_))'''"
      ],
      "metadata": {
        "id": "sc9mBk72VFNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sairam\")"
      ],
      "metadata": {
        "id": "-ArmncPPW7FY",
        "outputId": "bdd433f6-96fe-4c2a-aa3d-964492c69901",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sairam\n"
          ]
        }
      ]
    }
  ]
}